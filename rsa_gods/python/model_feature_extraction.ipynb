{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading CLIP-RN50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"RN50\", device=device)\n",
    "\n",
    "\n",
    "#loading in the attention pool layer\n",
    "model.visual.attnpool = model.visual.attnpool.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining the function for extracting the model representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get latent visual representations for an image\n",
    "def get_latent_representation(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image_input = preprocess(image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    return image_features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  os.path import join as pjoin\n",
    "def get_batch_latent_representation(\n",
    "        image_base_dir : str,\n",
    "        image_path: list\n",
    "):\n",
    "    #for each image in the list \n",
    "    #preprocess and add into a tensor\n",
    "    image_batch=[preprocess(Image.open(pjoin(image_base_dir,image_path))) for image_path in image_path]\n",
    "    image_batch = torch.stack(image_batch).to(device)\n",
    "    #get the latent representation\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_batch)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    return image_features.cpu().numpy()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the function for extracting the intermediary functions\n",
    "attnpool_output={}\n",
    "def get_intermediate_output(module,input,output):\n",
    "    attnpool_output['attnpool']=output\n",
    "#regiter the hook\n",
    "attnpool_layer=model.visual.attnpool\n",
    "attnpool_layer.register_forward_hook(get_intermediate_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining fucntion to extract the representation\n",
    "def extract_attnpool_repr(\n",
    "    image_base_dir : str,\n",
    "    image_path: list\n",
    "):\n",
    "    #for each image in the list \n",
    "    #preprocess and add into a tensor\n",
    "    image_batch=[preprocess(Image.open(pjoin(image_base_dir,image_path))) for image_path in image_path]\n",
    "    image_batch = torch.stack(image_batch).to(device)\n",
    "    \n",
    "    #get the latent representation\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_batch)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        #extract the output of the attention pool layer\n",
    "    return attnpool_output['attnpool'].cpu().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the image data filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_path\n",
    "image_dir='/DATA1/satwick22/Documents/fMRI/multimodal_concepts/images'\n",
    "#load the numpy array of image\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "image_names=np.load(pjoin(image_dir,'perceptionTest-image_filenames.npy'),allow_pickle=True)\n",
    "test_image_dir=pjoin(image_dir,'test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the attention pool representation\n",
    "image_repr_attn=extract_attnpool_repr(test_image_dir,image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file=pjoin(test_image_dir,image_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model and preprocessing function\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"RN50\", device=device)\n",
    "\n",
    "# Dictionary to store the outputs of intermediate layers\n",
    "intermediate_outputs = {}\n",
    "\n",
    "# Function to save intermediate outputs\n",
    "def get_intermediate_output(module, input, output):\n",
    "    intermediate_outputs['attnpool'] = output\n",
    "\n",
    "# Register the hook\n",
    "attnpool_layer = model.visual.attnpool\n",
    "attnpool_layer.register_forward_hook(get_intermediate_output)\n",
    "\n",
    "# Function to get latent visual representations for an image\n",
    "def get_attnpool_representation(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image_input = preprocess(image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "    with torch.no_grad():\n",
    "        model.encode_image(image_input)\n",
    "    return intermediate_outputs['attnpool'].cpu().numpy()\n",
    "\n",
    "# Example usage\n",
    "attnpool_representation = get_attnpool_representation(img_file)\n",
    "print(attnpool_representation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attnpool_representation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting latent representations of each image individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read one image\n",
    "img_file=pjoin(test_image_dir,image_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_repr=get_latent_representation(img_file)\n",
    "print(latent_repr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the image representations for all test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send the list and get the latent representation\n",
    "image_repr=get_batch_latent_representation(test_image_dir,image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_repr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the function for saving the images in disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_representation(bidsrooot:str,\n",
    "                        outdirname:str,\n",
    "                        image_batch:np.ndarray):\n",
    "    \"\"\"\n",
    "    Gets 2d tensors as representations and saves it in the disk\"\"\"\n",
    "    import os\n",
    "    save_path_dir=pjoin(bidsrooot,'derivatives','model-representations',outdirname)\n",
    "    if not os.path.exists(save_path_dir):\n",
    "        os.makedirs(save_path_dir)\n",
    "    out_file=pjoin(save_path_dir,f'PerceptionTest-image_representation.npy')\n",
    "    np.save(out_file,image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidsroot='/DATA1/satwick22/Documents/fMRI/multimodal_concepts/generic_object_decoding_bids'\n",
    "outdirname='model-representations/CLIP/attention_pool'\n",
    "save_representation(bidsroot,outdirname,image_repr_attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Virtex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.hub.load(\"kdexd/virtex\", \"resnet50\", pretrained=True)\n",
    "\n",
    "# This is a torchvision-like resnet50 model, with ``avgpool`` and ``fc``\n",
    "# layers replaced with ``nn.Identity`` module.\n",
    "image_batch = torch.randn(1, 3, 224, 224)  # batch tensor of one image.\n",
    "features_batch = model(image_batch)  # shape: (1, 2048, 7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hook onto the avgpool layer\n",
    "def create_forward_hook(module):\n",
    "    def hook(module, input, output):\n",
    "        hook.output = output.detach()\n",
    "    hook.output = None\n",
    "    module.register_forward_hook(hook)\n",
    "    return hook\n",
    "# output={}\n",
    "# def hook_fn(module,input,output):\n",
    "#     output['layer']=output\n",
    "\n",
    "\n",
    "# def get_latent_representation(image_path):\n",
    "#     image = Image.open(image_path)\n",
    "#     image_input = preprocess(image).unsqueeze(0)\n",
    "#     with torch.no_grad():\n",
    "#         model(image_input)\n",
    "#     return output['layer'].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load(\"kdexd/virtex\", \"resnet50\", pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "hook= create_forward_hook(model.avgpool)\n",
    "\n",
    "# This is a torchvision-like resnet50 model, with ``avgpool`` and ``fc``\n",
    "# layers replaced with ``nn.Identity`` module.\n",
    "image_batch = torch.randn(1, 3, 224, 224)  # batch tensor of one image.\n",
    "features_batch = model(image_batch)  # shape: (1, 2048, 7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_representation(\n",
    "        image_base_dir : str,\n",
    "        image_path : list\n",
    "):\n",
    "    from torchvision import transforms\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    #get the preprocessed image\n",
    "    image_batch=[preprocess(Image.open(pjoin(image_base_dir,image_path))) for image_path in image_path]\n",
    "    image_batch = torch.stack(image_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(image_batch)\n",
    "    return hook.output.cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#image_path\n",
    "image_dir='/DATA1/satwick22/Documents/fMRI/multimodal_concepts/images'\n",
    "#load the numpy array of image\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "image_names=np.load(pjoin(image_dir,'perceptionTest-image_filenames.npy'),allow_pickle=True)\n",
    "test_image_dir=pjoin(image_dir,'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_repr_avgpool=get_latent_representation(test_image_dir,image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_repr_avgpool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the image representations\n",
    "outdirname='virtex/avgpool'\n",
    "save_representation(bidsroot,outdirname,image_repr_avgpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions for saving the representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for creating the forward hook\n",
    "def create_forward_hook(module):\n",
    "    def hook(module, input, output):\n",
    "        hook.output = output.detach()\n",
    "    hook.output = None\n",
    "    module.register_forward_hook(hook)\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fucntion for extracting the latent representation from the hooked layer\n",
    "def extract_representation(model,pool_layer,preprocess,image_base_dir,image_path):\n",
    "    #hook onto the layer\n",
    "    hook= create_forward_hook(pool_layer)\n",
    "    #get the preprocessed image\n",
    "    image_batch=[preprocess(Image.open(pjoin(image_base_dir,image_path))) for image_path in image_path]\n",
    "    image_batch = torch.stack(image_batch)\n",
    "    #move the image to the device\n",
    "    image_batch=image_batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(image_batch)\n",
    "    return hook.output.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in the data file for images\n",
    "#image_path\n",
    "image_dir='/DATA1/satwick22/Documents/fMRI/multimodal_concepts/images'\n",
    "#load the numpy array of image\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "image_names=np.load(pjoin(image_dir,'perceptionTest-image_filenames.npy'),allow_pickle=True)\n",
    "test_image_dir=pjoin(image_dir,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for saving the representation\n",
    "\n",
    "def save_representation(bidsrooot:str,\n",
    "                        outdirname:str,\n",
    "                        image_batch:np.ndarray):\n",
    "    \"\"\"\n",
    "    Gets 2d tensors as representations and saves it in the disk\"\"\"\n",
    "    import os\n",
    "    save_path_dir=pjoin(bidsrooot,'derivatives','model-representations',outdirname)\n",
    "    if not os.path.exists(save_path_dir):\n",
    "        os.makedirs(save_path_dir)\n",
    "    out_file=pjoin(save_path_dir,f'PerceptionTest-image_representation.npy')\n",
    "    np.save(out_file,image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in the important directories\n",
    "\n",
    "bidsroot='/DATA1/satwick22/Documents/fMRI/multimodal_concepts/generic_object_decoding_bids'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Virtex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load(\"kdexd/virtex\", \"resnet50\", pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print('Model loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the model avgpool layer reference\n",
    "pool_layer=model.avgpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the latent representation\n",
    "image_repr_avgpool=extract_representation(model,pool_layer,preprocess,test_image_dir,image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the image representations\n",
    "outdirname='virtex/avgpool'\n",
    "save_representation(bidsroot,outdirname,image_repr_avgpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Resnet 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the model object\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print('Model loaded succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the avgpool layer\n",
    "avgpool_layer = model.avgpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the function to extract the latent representation\n",
    "image_repr_avgpool=extract_representation(model,avgpool_layer,preprocess,test_image_dir,image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_repr_avgpool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the image representations\n",
    "save_representation(bidsroot,'resnet/avgpool',image_repr_avgpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For BiT-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "import timm  \n",
    "# Load the pretrained BiT-M model\n",
    "model = timm.create_model('resnetv2_50x1_bitm', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print('Model loaded successfully')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the target layer object\n",
    "head_avg_layer = model.get_submodule('head.global_pool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the preprocess function\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Mean and std for BiT-M\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the function to extract the latent representation\n",
    "image_repr_avgpool=extract_representation(model,head_avg_layer,preprocess,test_image_dir,image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the image representations\n",
    "save_representation(bidsroot,'BiT-M/head.avg',image_repr_avgpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For TSM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/DATA1/satwick22/Documents/fMRI/multimodal_concepts/code/temporal-shift-module')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ops.models import TSN\n",
    "from ops.transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TSM ResNet-50 model\n",
    "num_class = 400  # Number of classes, e.g., 400 for Kinetics-400\n",
    "model = TSN(num_class, 1, 'RGB',\n",
    "            base_model='resnet50',\n",
    "            consensus_type='avg',\n",
    "            img_feature_dim=256,\n",
    "            print_spec=False,\n",
    "            pretrain='imagenet',\n",
    "            is_shift=True, shift_div=8, shift_place='blockres',\n",
    "            fc_lr5=True)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the model\n",
    "from pytorchvideo.models.hub import tsm_resnet50\n",
    "#from pytorchvideo.models.hub import tsm_resnet50\n",
    "# Load the pretrained TSM ResNet-50 model\n",
    "model = tsm_resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For ICMLM-attfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import torch.nn as nn\n",
    "class ICMLMattFCModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ICMLMattFCModel, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            # Add more layers to match the 'cnn' part of the state_dict if needed\n",
    "        )\n",
    "        self.step = nn.Sequential(\n",
    "            nn.Linear(64 * 56 * 56, 512),  # Assuming the output of cnn is 64x56x56\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1000)  # Adjust the output dimension based on the actual model\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.step(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "#model_att_fc = ICMLMattFCModel()\n",
    "from os.path import join as pjoin\n",
    "model_att_fc=torch.load(pjoin('/DATA1/satwick22/Documents/fMRI/multimodal_concepts/models','icmlm-attfc_r50_coco_5K.pth'))\n",
    "#model_att_fc.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_att_fc.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict=torch.load(pjoin('/DATA1/satwick22/Documents/fMRI/multimodal_concepts/models','icmlm-attfc_r50_coco_5K.pth'))\n",
    "print(state_dict['cnn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the model architecture\n",
    "class ICMLMattFCModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ICMLMattFCModel, self).__init__()\n",
    "        \n",
    "        # Assuming `cnn` is a pre-trained model, like ResNet50\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            # Add more layers to match the actual architecture...\n",
    "        )\n",
    "        \n",
    "        # Define the attention mechanism\n",
    "        self.attention = nn.Linear(in_features=512, out_features=512)  # Example dimensions\n",
    "        self.fc = nn.Linear(in_features=512, out_features=10)  # Example output dimensions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        \n",
    "        # Flatten the output from CNN\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        x = self.attention(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = ICMLMattFCModel()\n",
    "\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# Now you can use the model to extract representations\n",
    "input_image = torch.randn(1, 3, 224, 224)  # Example input image\n",
    "output_representation = model(input_image)\n",
    "\n",
    "print(output_representation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Adversarially Trained Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from robustness.model_utils import make_and_restore_model\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "base_weight_path='/DATA1/satwick22/Documents/fMRI/multimodal_concepts/models/AR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dummy dataset object for loading the model\n",
    "from robustness import datasets\n",
    "\n",
    "class DummyImageNet(datasets.ImageNet):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__('/path/to/dummy/dataset')\n",
    "\n",
    "dummy_dataset = DummyImageNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For ARL2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the weights path\n",
    "weight_path = pjoin(base_weight_path, 'imagenet_l2_3_0.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "# Load the model\n",
    "model, _ = make_and_restore_model(arch='resnet50', dataset=dummy_dataset, resume_path=weight_path)\n",
    "model.eval()\n",
    "model = model.model  # Extract the inner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing transformation\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=dummy_dataset.mean.tolist(), std=dummy_dataset.std.tolist()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the avgpool layer\n",
    "avgpool_layer = model.avgpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the representations from the avgpool layer\n",
    "image_repr_avgpool=extract_representation(model,avgpool_layer,preprocess,test_image_dir,image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_representation(bidsroot,'madryl23/avgpool',image_repr_avgpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For AR linf , $ \\epsilon = 8/255$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the weight path\n",
    "weight_path = pjoin(base_weight_path, 'imagenet_linf_8.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the model\n",
    "model, _ = make_and_restore_model(arch='resnet50', dataset=dummy_dataset, resume_path=weight_path)\n",
    "model.eval()\n",
    "model = model.model  # Extract the inner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "print(\"Model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the preprocessing transformation\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=dummy_dataset.mean.tolist(), std=dummy_dataset.std.tolist()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgpool_layer = model.avgpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the representations from the avgpool layer\n",
    "image_repr_avgpool=extract_representation(model,avgpool_layer,preprocess,test_image_dir,image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_representation(bidsroot,'madryli8/avgpool',image_repr_avgpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For AR linf , $ \\epsilon = 4/255$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path=pjoin(base_weight_path,'imagenet_linf_4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the model\n",
    "model, _ = make_and_restore_model(arch='resnet50', dataset=dummy_dataset, resume_path=weight_path)\n",
    "model.eval()\n",
    "model = model.model  # Extract the inner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move the model to the device\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "print(\"Model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing transformation\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=dummy_dataset.mean.tolist(), std=dummy_dataset.std.tolist()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the avgpool layer\n",
    "avgpool_layer = model.avgpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the representations from the avgpool layer\n",
    "image_repr_avgpool=extract_representation(model,avgpool_layer,preprocess,test_image_dir,image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the image representations\n",
    "save_representation(bidsroot,'madryli4/avgpool',image_repr_avgpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Stylized ImageNet Trained Resnet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the path of the models \n",
    "import sys\n",
    "sys.path.append('/DATA1/satwick22/Documents/fMRI/multimodal_concepts/texture-vs-shape/models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SIN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the model\n",
    "from load_pretrained_models import load_model\n",
    "model_name=\"resnet50_trained_on_SIN\"\n",
    "model=load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move the model to device\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing transformation\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the avgpool layer\n",
    "avgpool_layer = model.module.avgpool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the representations from the avgpool layer\n",
    "image_repr_avgpool=extract_representation(model,avgpool_layer,preprocess,test_image_dir,image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the image representations\n",
    "save_representation(bidsroot,'geirhos_sin/avgpool',image_repr_avgpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For SIN_IN trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import the model\n",
    "from load_pretrained_models import load_model\n",
    "model_name=\"resnet50_trained_on_SIN_and_IN\"\n",
    "model=load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move the model to device\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing transformation\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the avgpool layer\n",
    "avgpool_layer = model.module.avgpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the representations from the avgpool layer\n",
    "image_repr_avgpool=extract_representation(model,avgpool_layer,preprocess,test_image_dir,image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the image representations\n",
    "save_representation(bidsroot,'geirhos_sinin/avgpool',image_repr_avgpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For SIN_IN_IN trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "\n",
    "from load_pretrained_models import load_model\n",
    "model_name=\"resnet50_trained_on_SIN_and_IN_then_finetuned_on_IN\"\n",
    "model=load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#move the model to device\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing transformation\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the avgpool layer\n",
    "avgpool_layer = model.module.avgpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#extract the representations from the avgpool layer\n",
    "image_repr_avgpool=extract_representation(model,avgpool_layer,preprocess,test_image_dir,image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the image representations\n",
    "save_representation(bidsroot,'geirhos_sininfin/avgpool',image_repr_avgpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2Model.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer the model to the device\n",
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(texts,tokenizer):\n",
    "    # Tokenize and pad the texts to the same length\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Move inputs to the GPU\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    return inputs\n",
    "# Set the padding token as the eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "text_inputs=preprocess_texts(text_input,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable gradient calculation for efficiency\n",
    "with torch.no_grad():\n",
    "    outputs = model(**text_inputs)\n",
    "\n",
    "# Extract the hidden states\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "# Average the hidden states to get a single representation per input\n",
    "latent_representations = last_hidden_state.mean(dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_representations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the image representations\n",
    "save_representation(bidsroot,'GPT2/avg',latent_representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Move the model to the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Model loaded successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_texts(texts):\n",
    "    # Tokenize and pad the texts to the same length\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    # Move inputs to the GPU\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    return inputs\n",
    "text_inputs = preprocess_texts(text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable gradient calculation for efficiency\n",
    "with torch.no_grad():\n",
    "    outputs = model(**text_inputs)\n",
    "\n",
    "# Extract the hidden states\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "# Average the hidden states to get a single representation per input\n",
    "latent_representations = last_hidden_state.mean(dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_representation(bidsroot,'BERT/avg',latent_representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For CLIP-L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "\n",
    "# Load the CLIP model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, preprocess = clip.load(\"\", device=device)\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    # Tokenize the texts using CLIP's tokenizer\n",
    "    inputs = clip.tokenize(texts).to(device)\n",
    "    return inputs\n",
    "\n",
    "# Example batch of texts\n",
    "text_inputs = preprocess_texts(text_input)\n",
    "\n",
    "# Disable gradient calculation for efficiency\n",
    "with torch.no_grad():\n",
    "    # Get the text embeddings\n",
    "    text_embeddings = model.encode_text(text_inputs).cpu().numpy()\n",
    "\n",
    "#save the image representations\n",
    "save_representation(bidsroot,'CLIP-L/avg',text_embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
