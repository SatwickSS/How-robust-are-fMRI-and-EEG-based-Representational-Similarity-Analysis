{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "#read a pickle file\n",
    "n_shuffle_offset=0\n",
    "n_shuffle=20\n",
    "sub_id='01'\n",
    "with open(f\"/path/to/shuffle.pkl\",'rb') as f:\n",
    "    results=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_string(s):\n",
    "    \"\"\"\n",
    "    Parse underscore-separated string into a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        s (str): Input string in format 'centered_by_psc_recentered_by_psc_zscored_1_hrf_by_mean_6_runs_intersection'\n",
    "    \n",
    "    Returns:\n",
    "        dict: Parsed parameters\n",
    "    \"\"\"\n",
    "    parts = s.split('_')\n",
    "    result = {}\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(parts):\n",
    "        # Handle 'xxx_by_yyy' pattern\n",
    "        if i + 2 < len(parts) and parts[i + 1] == \"by\":\n",
    "            key = parts[i]\n",
    "            value = parts[i + 2]\n",
    "            result[key] = value\n",
    "            i += 3\n",
    "        # Handle 'key_value' pattern\n",
    "        elif i + 1 < len(parts):\n",
    "            key = parts[i]\n",
    "            value = parts[i + 1]\n",
    "            # Try converting to int if possible\n",
    "            try:\n",
    "                value = int(value)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            result[key] = value\n",
    "            i += 2\n",
    "        # Handle last unpaired item\n",
    "        else:\n",
    "            result['mask_method'] = parts[i]\n",
    "            i += 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary of specifications and store the scores for computing p-values\n",
    "def score_organizer(samples_list:list):\n",
    "    scores_dict={}\n",
    "    for sample_ind,sample in enumerate(samples_list):\n",
    "        for spec_score_config in sample.keys():\n",
    "            #check if the specification is already in the dictionary\n",
    "            #if not create a new key and store the score\n",
    "            if not sample_ind:\n",
    "                #create the empty dictionary for the current specification\n",
    "                scores_dict[spec_score_config]={'mean':[]}\n",
    "                scores_dict[spec_score_config]['vanilla_data']=sample[spec_score_config]\n",
    "            else: scores_dict[spec_score_config]['mean'].append(sample[spec_score_config])\n",
    "            #scores_dict[sample['specifications']]=[]\n",
    "        #scores_dict[sample[spec_score_config]].append(sample['scores'])\n",
    "    return scores_dict\n",
    "spec_scores=score_organizer(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_parameter_string(input_string):\n",
    "    \"\"\"\n",
    "    Parse a parameter string of the format 'centered_by_psc_recentered_by_psc_zscored_1_hrf_by_mean_6_runs_intersection'\n",
    "    into a dictionary with corresponding keys and values.\n",
    "    \n",
    "    Args:\n",
    "        input_string (str): The input string to parse\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing the parsed parameters\n",
    "    \"\"\"\n",
    "    # Initialize empty dictionary for results\n",
    "    params = {}\n",
    "    \n",
    "    # Split the string by underscore\n",
    "    parts = input_string.split('_')\n",
    "    \n",
    "    # Initialize index to iterate through parts\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(parts):\n",
    "        # Handle centered_by\n",
    "        if parts[i] == 'centered' and parts[i+1] == 'by':\n",
    "            params['centered_by'] = parts[i+2]\n",
    "            i += 3\n",
    "        \n",
    "        # Handle recentered_by\n",
    "        elif parts[i] == 'recentered' and parts[i+1] == 'by':\n",
    "            params['recentered_by'] = parts[i+2]\n",
    "            i += 3\n",
    "            \n",
    "        # Handle zscored\n",
    "        elif parts[i] == 'zscored':\n",
    "            params['zscored'] = \"True\" if int(parts[i+1]) else \"False\"\n",
    "            i += 2\n",
    "            \n",
    "        # Handle hrf_by\n",
    "        elif parts[i] == 'hrf' and parts[i+1] == 'by':\n",
    "            params['hrf_by'] = parts[i+2]\n",
    "            i += 3\n",
    "            \n",
    "        # Handle runs\n",
    "        elif parts[i] == 'runs':\n",
    "            params['number_of_runs'] = f\"{parts[i-1]}-runs\"  # Get the number before 'runs'\n",
    "            i += 1\n",
    "            \n",
    "        # Handle mask_method (last part)\n",
    "        elif i == len(parts) - 1:\n",
    "            params['mask_method'] = parts[i]\n",
    "            i += 1\n",
    "            \n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_parameter_string('centered_by_psc_recentered_by_psc_zscored_1_hrf_by_mean_6_runs_intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the p-values for each analysis specification for each sample\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "p_values={}\n",
    "org_spec_list=[]\n",
    "percentiles={}\n",
    "for spec in spec_scores.keys():\n",
    "    spec_dict=parse_parameter_string(spec)\n",
    "    # if spec_dict[\"recentered_by\"]==\"psc\" and spec_dict[\"centered_by\"]!=\"off\":\n",
    "    #     continue\n",
    "    p_values[spec]={'all_samples':[],'vanilla_data':None}\n",
    "    #extract the sample correlation averages for each specification\n",
    "    sample_scores=spec_scores[spec]['mean']+[spec_scores[spec]['vanilla_data']]\n",
    "    sample_scores_nparray=np.array(sample_scores)\n",
    "    #iterate through the sample scores\n",
    "    for i in range(len(sample_scores)):\n",
    "        #calulate the p-value\n",
    "        if spec=='centered_by_psc_recentered_by_re-center_zscored_1_hrf_by_mean_6_runs_intersection':\n",
    "            p_val=0\n",
    "        else:\n",
    "            p_val_var=stats.percentileofscore(sample_scores_nparray,sample_scores[i])/100\n",
    "            p_val=2*(min(p_val_var,1-p_val_var))\n",
    "        p_values[spec]['all_samples']+=[p_val]\n",
    "    if spec=='centered_by_psc_recentered_by_re-center_zscored_1_hrf_by_mean_6_runs_intersection':\n",
    "        p_values[spec]['vanilla_data']=0\n",
    "        continue\n",
    "    percentile_ratio=(stats.percentileofscore(sample_scores_nparray,spec_scores[spec]['vanilla_data'])/100)\n",
    "\n",
    "    p_values[spec]['vanilla_data']=2*(min(percentile_ratio, 1 - percentile_ratio))\n",
    "    # percentiles[spec]=percentile*100\n",
    "    # p_values[spec]['vanilla_data']=fun(percentile)\n",
    "    spec_dict=parse_parameter_string(spec)\n",
    "    spec_dict['p-value']=p_val\n",
    "    org_spec_list.append(spec_dict)\n",
    "#p_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=[]\n",
    "for spec_combs in p_values:\n",
    "    spec_dict=parse_parameter_string(spec_combs)\n",
    "    spec_dict[\"significance\"]=p_values[spec_combs][\"vanilla_data\"]\n",
    "    rows.append(spec_dict)\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(rows)\n",
    "df.rename(columns={'centered_by': 'centering', 'recentered_by': 're-centering', 'zscored': 'zscoring_residuals', 'hrf_by': 'hrf_method', 'number_of_runs': 'runs_used', 'significance': 'significant'}, inplace=True)\n",
    "# df['zscoring_residuals'] = df['zscoring_residuals'].astype(bool)\n",
    "df[\"significant\"]=np.where(df['significant'] < 0.025, 'Yes', 'No')\n",
    "# df['runs_used'] = df['runs_used'].map(lambda x: f\"{x}-runs\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through the dictionary and extract the pvalues for vanilla data\n",
    "spec_scores_vanilla=[]\n",
    "for ind,spec in enumerate(p_values):\n",
    "    spec_scores_vanilla.append(p_values[spec]['vanilla_data'])\n",
    "    if spec=='centered_by_psc_recentered_by_off_zscored_1_hrf_by_mean_6_runs_intersection':\n",
    "        og_index=ind\n",
    "\n",
    "sorted_index=np.argsort(spec_scores_vanilla)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN To get decision tree ds\n",
    "# rows=[]\n",
    "# for spec_combs in p_values:\n",
    "#     spec_dict=parse_parameter_string(spec_combs)\n",
    "#     spec_dict[\"significance\"]=p_values[spec_combs][\"vanilla_data\"]\n",
    "#     rows.append(spec_dict)\n",
    "# import pandas as pd\n",
    "# df=pd.DataFrame(rows)\n",
    "# df.rename(columns={'centered_by': 'centering', 'recentered_by': 're-centering', 'zscored': 'zscoring_residuals', 'hrf_by': 'hrf_method', 'number_of_runs': 'runs_used', 'significance': 'significant'}, inplace=True)\n",
    "# # df['zscoring_residuals'] = df['zscoring_residuals'].astype(bool)\n",
    "# df[\"significant\"]=np.where(df['significant'] < 0.025, 'Yes', 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(spec_scores_vanilla)<0.025).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "# Main p-values line with increased linewidth\n",
    "plt.plot(np.array(spec_scores_vanilla)[sorted_index], \n",
    "         linewidth=3,  # Increased line thickness\n",
    "         label='p-values')\n",
    "\n",
    "# Significance level lines with increased linewidth\n",
    "# plt.axhline(y=0.975, color='purple', linestyle='--', \n",
    "#             linewidth=3,  # Increased line thickness\n",
    "#             label='0.975 significance level')\n",
    "plt.axhline(y=0.025, color='magenta', linestyle='--', \n",
    "            linewidth=3,  # Increased line thickness\n",
    "            label='0.025 significance level')\n",
    "\n",
    "plt.xlabel('Specifications(sorted by p-values)', fontsize=20)\n",
    "plt.ylabel('p-values', fontsize=20)\n",
    "\n",
    "# Scatter plot with larger marker size\n",
    "plt.scatter(og_index, 0, \n",
    "           c='r', s=150,  # Increased marker size\n",
    "           marker='o', label='Original Specification')\n",
    "\n",
    "plt.legend(fontsize=11, loc='upper left', bbox_to_anchor=(0, 0.95))\n",
    "plt.box(on=True)\n",
    "\n",
    "# Thicker tick marks and spines\n",
    "plt.tick_params(axis='both', which='major', labelsize=20, width=3)  # Increased tick width\n",
    "\n",
    "# Make the plot border (spines) thicker\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_linewidth(3)  # Thicker border\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/satwick22/Documents/plots/spec_curv_fmri_6_4.pdf', \n",
    "            dpi=300, \n",
    "            bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
